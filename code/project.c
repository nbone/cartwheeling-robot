#pragma config(Sensor, S1,			 toe1,			sensorTouch)
#pragma config(Sensor, S2,			 toe2,			sensorTouch)
#pragma config(Sensor, S3,			 toe3,			sensorTouch)
#pragma config(Motor,	motorA,		leg1,			tmotorNormal, PIDControl, encoder)
#pragma config(Motor,	motorB,		leg2,			tmotorNormal, PIDControl, encoder)
#pragma config(Motor,	motorC,		leg3,			tmotorNormal, PIDControl, encoder)
//*!!Code automatically generated by 'ROBOTC' configuration wizard	!!*//

/* NOTE: using PID control causes motors to lock and spasm even after setting power to zero and bFloatDuringInactiveMotorPWM to true (this is a bug in RobotC), therefore it is necessary to manually turn off PID on individual motors to put them in neutral by setting nMotorPIDSpeedCtrl[motor] = mtrNoReg (and mtrSpeedReg to turn PID back on).  Why use PID at all? because the motors do not receive sufficient power to overcome the weight of the legs and body (depending on current limb position). */

/* CS572 - Robotics
 * Final Project
 * 25 May 2010
 * Nicholas Bone
 */

/* Plan:
 * Robot that learns to walk.
 * Three legs; symmetric; moves cartwheel fashion.
 * Each leg has a touch sensor ("toe") on the end to detect when it's on the ground (but won't fire at severe angles).
 * Each leg has a motor at the hip, with approximately 180 degrees of freedom.
 * The current rotation angles of each leg is important proprioceptive input for the robot to decide its next action.
 * Use Reinforcement Learning:
 * Reward = sequence of touch configs: (1,0,0) -> (1,1,0) -> (0,1,0) -> (0,1,1) -> (0,0,1) -> (1,0,1) -> repeat...
 * State at each moment = (t1,r1,t2,r2,t3,r3), where t* = {0,1} and r* = [0,180].
 * Because of symmetry, can triple each learning instance by shifting 1 -> 2 -> 3 -> 1
 * Action is power to give each motor.
 * Two main options:
 *	(1) a reactive robot, that acts only upon it's current input.
 *	(2) a lookahead robot, that acts towards a desirable state.
 * I think option (2) might be faster in this case, and is reasonable because robot can approximately predict next state
 * by estimating that all touch sensors will be unchanged, and each rotation sensor will be updated by some time delta
 * multiplied by the power sent to that motor.
 * Let's try Q-Learning first.  We can also consider GA used with ANN representation.
 * State-space is rather large, unless we bucket the rotation values.
 * Q-Learning requires that we learn to estimate the "value" of each state, which works best when the state-space is
 * continuous (which it is in this case) and not too large (it's sort of medium sized here).
 * We either need to store the entire state-table in memory, or approximate it using some FA like an ANN.
 * For efficiency of training, use eligibility traces for learning updates, i.e., track the most recent states
 * (perhaps the last 100?) and give each a decayed reward when we achieve the next target.  One way to do this would
 * be to keep a copy of the whole state table, and at each step set the current state to 1 and multiply each other
 * entry by some decay factor like 0.96.  Then when we hit a reward (say, 100), adjust the value of each state towards
 * its eligibility trace factor times the reward value.  But if the state space is very large better to use a fixed-size list.
 *
 * Revised plan, after much thought:
 * - Learn a state-value function, approximated by an ANN.
 * - 15 inputs: 5 gaussians (e^(-B*|x-c|)) for each leg, with evenly spaced centers (0.0, 0.25, 0.5, 0.75, 1.0) and B = 4.
 * - This input representation will give the following input activations for these values of x:
 *			x = 0.00	0.125	0.25	0.375	0.50	0.625	0.75	0.875	1.00
 *	c =	0.00:	1.00	0.61	0.37	0.22	0.14	0.08	0.05	0.03	0.02
 *		0.25:	0.37	0.61	1.00	0.61	0.37	0.22	0.14	0.08	0.05
 *		0.50:	0.14	0.22	0.37	0.61	1.00	0.61	0.37	0.22	0.14
 *		0.75:	0.05	0.08	0.14	0.22	0.37	0.67	1.00	0.67	0.37
 *		1.00:	0.02	0.03	0.05	0.08	0.14	0.22	0.37	0.61	1.00
 * - 3 hidden inputs with sigmoid activation.
 * - 1 output with sigmoid activation (1.0 = reward state, 0.0 = very bad state).
 * - Train two copies of the ANN, one for each state:
 *	 (1) One toe touching (translated so inputs 1-5 correspond to the touching toe).
 *	 (2) Two toes touching (translates so inputs 1-10 correspond to the touching toes).
 * - NOTE: this means we don't need to explicitly represent the touch sensors in the input.
 * - NOTE: for simplicity, we're ignoring the affect of momentum, which implies that the robot should be able to walk slow as well as fast.
 * - REWARDS:
 *	 > When one toe is down, give a maximum reward when the next toe touches.
 *	 > When one toe is down, give a minimum reward when the previous toe touches (punish).
 *	 > When two toes are down, give a maximum reward when the back toe lifts.
 *	 > When two toes are down, give a minimum reward when the front toe lifts (punish).
 * - TRAINING:
 *	 > Have a manual training mode with motors off where robot feels human moving its legs.
 *	 > Each state encountered during training gets a moderate (75%) reward, except when overridden by touch signals.
 *	 > These TRAINING rewards are "positive only", meaning that they have no effect if the state value is already above the reward level.
 *	 > The LEARNING rewards still take effect, as below, and override the special TRAINING rewards.
 * - LEARNING:
 *	 > At each step, train the previous state's value estimation using the current state's value times some decay factor (e.g. 0.98).
 *	 > This, combined with the TRAINING phase, means we don't really need eligibility traces.
 *	 > When receiving a reward or punishment, train the current state and the previous using the same value.
 *	 > After a reward or punishment, we switch to the other learner (different number of toes), so the state will have a different value.
 * - ACTION SELECTION:
 *	 > For simplicity, consider each leg separately.
 *	 > At each step, consider applying a power of {-100, -90, ..., 0, 10, ..., 90, 100} to the leg and estimate the resultant angle.
 *	 > Choose the action that leads to the best value estimate.
 *	 > Repeat this selection for all three legs, and then enact the powers for all three simultaneously.
 *	 > To estimate the angle from the power, we need to know approximately how many steps per second, and the relation between power and rotation speed.
 */


// NOTE: RobotC seems to "lose" return values from deeply nested function calls


// Unfortunately the "extra" ifndefs below are necessary because the compiler is stupid
#ifndef CONSTANTS_H
#include "constants.h"
#endif
#ifndef ROBOT_C
#include "robot.c"	// no linker, so must include .c instead of .h
#endif


void Setup() {
	nVolume = 1;	// lowest audible volume
	nNxtExitClicks = 3; // number of consecutive clicks of exit button to abort program
	InitSigmoidLookupTable();
}

void WaitForButtonRelease() {
	while (nNxtButtonPressed != kNoButton) {
		wait1Msec(10);
	}
}

int WaitGetButtonPress() {
	while (nNxtButtonPressed == kNoButton) {
		wait1Msec(10);
	}
	return nNxtButtonPressed;
}

void TurnOffMotors() {
	bFloatDuringInactiveMotorPWM = true;
	motor[leg1] = 0;
	motor[leg2] = 0;
	motor[leg3] = 0;
	wait1Msec(200);
}

void CalibrateLeg(RobotSettings& rs, int whichLeg, int whichToe) {
	bFloatDuringInactiveMotorPWM = true;
	nxtDisplayTextLine(1, "Bend to MIN and tap");
	// Wait for release and retouch:
	while (1 == SensorValue[whichToe]);
	while (0 == SensorValue[whichToe]);// nxtDisplayCenteredTextLine(5, "%d", nMotorEncoder[whichLeg]);
	PlayTone(TONE_NOTIFY, 10);
	// First touch = minimum leg angle:
	nMotorEncoder[whichLeg] = 0;
	int angleMin = nMotorEncoder[whichLeg];
	nxtDisplayTextLine(1, "Bend to MAX and tap");
	// Wait for release and retouch:
	while (1 == SensorValue[whichToe]);
	while (0 == SensorValue[whichToe]);// nxtDisplayCenteredTextLine(5, "%d", nMotorEncoder[whichLeg]);
	PlayTone(TONE_NOTIFY, 10);
	// Second touch = maximum leg angle:
	int angleMax = nMotorEncoder[whichLeg];
	// Swap min/max if necessary (if user did it backwards):
	if (nMotorEncoder[whichLeg] < 0) {
		nMotorEncoder[whichLeg] = 0;
		angleMax = -angleMax;
	}
	// Wait for release:
	while (1 == SensorValue[whichToe]);
	// Transfer min/max values to settings:
	switch (whichLeg) {
		case leg1:
			rs.leg1min = angleMin;
			rs.leg1max = angleMax;
			break;
		case leg2:
			rs.leg2min = angleMin;
			rs.leg2max = angleMax;
			break;
		case leg3:
			rs.leg3min = angleMin;
			rs.leg3max = angleMax;
			break;
	}
}

// Procedure:
// - Wait until all three legs are calibrated.
// - Human touches a toe to start calibration of that leg.
// - Second touch of that toe indicates leg at minimum angle.
// - Third touch of that toe indicates leg at maximum angle.
// - Play beeps and display text to give feedback to human user.
void Calibrate(RobotSettings& rs) {
	TurnOffMotors();
	bool done1 = false;
	bool done2 = false;
	bool done3 = false;
	while (!done1 || !done2 || !done3) {
		nxtDisplayTextLine(1, "Tap a toe to set...");
		if (done1) {
			nxtDisplayTextLine(2, "Leg1: %d to %d", rs.leg1min, rs.leg1max);
		} else {
			nxtDisplayTextLine(2, "Leg1 not set");
		}
		if (done2) {
			nxtDisplayTextLine(3, "Leg2: %d to %d", rs.leg2min, rs.leg2max);
		} else {
			nxtDisplayTextLine(3, "Leg2 not set");
		}
		if (done3) {
			nxtDisplayTextLine(4, "Leg3: %d to %d", rs.leg3min, rs.leg3max);
		} else {
			nxtDisplayTextLine(4, "Leg3 not set");
		}
		if (SensorValue[toe1]) {
			nxtDisplayTextLine(2, "Leg1 calibrating...");
			PlayTone(TONE_NOTIFY, 20);
			wait1Msec(200);
			CalibrateLeg(rs, leg1, toe1);
			done1 = true;
		}
		if (SensorValue[toe2]) {
			nxtDisplayTextLine(3, "Leg2 calibrating...");
			PlayTone(TONE_NOTIFY, 20);
			wait1Msec(200);
			CalibrateLeg(rs, leg2, toe2);
			done2 = true;
		}
		if (SensorValue[toe3]) {
			nxtDisplayTextLine(4, "Leg3 calibrating...");
			PlayTone(TONE_NOTIFY, 20);
			wait1Msec(200);
			CalibrateLeg(rs, leg3, toe3);
			done3 = true;
		}
	}
	// Indicate we're done
	PlayTone(TONE_NOTIFY, 15);
	wait1Msec(200);
	PlayTone(TONE_NOTIFY, 15);
}

void FakeCalibrate(RobotSettings& rs) {
	nMotorEncoder[leg1] = 0;
	nMotorEncoder[leg2] = 0;
	nMotorEncoder[leg3] = 0;
	rs.leg1min = 0;
	rs.leg1max = 90;
	rs.leg2min = 0;
	rs.leg2max = 90;
	rs.leg3min = 0;
	rs.leg3max = 90;
}

void DisplayMainMenu() {
	nxtDisplayCenteredTextLine(1, "Main Menu:");
	nxtDisplayTextLine(2, "Left - Train mode");
	nxtDisplayTextLine(3, "Right - Auto mode");
	nxtDisplayTextLine(4, "Orange - Self-train");
	nxtDisplayTextLine(5, "Gray - Exit");
}

task main() {
	Setup();
	//Robot theRobot; *** this is now in global 'r_' variables ***
	RobotSettings theRobotSettings;
	R_Init();
//	FakeCalibrate(theRobotSettings);
	Calibrate(theRobotSettings);

	bool bRunning = true;
	while (bRunning) {
		TurnOffMotors();
		nMotorPIDSpeedCtrl[motorA] = mtrNoReg;
		nMotorPIDSpeedCtrl[motorB] = mtrNoReg;
		nMotorPIDSpeedCtrl[motorC] = mtrNoReg;
		DisplayMainMenu();
		WaitForButtonRelease();
		int button = WaitGetButtonPress();
		switch (button) {
			case kExitButton:
				bRunning = false;
				break;
			case kLeftButton:
				eraseDisplay();
				PlayTone(TONE_NOTIFY, 50);
				wait10Msec(50);
				R_SetTrainingMode(true);
				while (nNxtButtonPressed != kExitButton) {
					R_SenseActLearn(theRobotSettings);
				}
				break;
			case kRightButton:
				eraseDisplay();
				PlayTone(TONE_NOTIFY, 50);
				wait10Msec(50);
				R_SetTrainingMode(false);
				nMotorPIDSpeedCtrl[motorA] = mtrSpeedReg;
				nMotorPIDSpeedCtrl[motorB] = mtrSpeedReg;
				nMotorPIDSpeedCtrl[motorC] = mtrSpeedReg;
				while (nNxtButtonPressed != kExitButton) {
					R_SenseActLearn(theRobotSettings);
				}
				break;
			case kEnterButton:
				eraseDisplay();
				PlayTone(TONE_NOTIFY, 50);
				wait10Msec(50);
				R_SetHardcodeMode(true);
				nMotorPIDSpeedCtrl[motorA] = mtrSpeedReg;
				nMotorPIDSpeedCtrl[motorB] = mtrSpeedReg;
				nMotorPIDSpeedCtrl[motorC] = mtrSpeedReg;
				while (nNxtButtonPressed != kExitButton) {
					R_SenseActLearn(theRobotSettings);
				}
				break;
			default:
				break;
		}
	}

	StopAllTasks();
	nVolume = 0;
	return;
}
